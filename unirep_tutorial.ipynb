{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the UniRep mLSTM \"babbler\". This version demonstrates the 1900-unit architecture. On a laptop, unirep_tutorial_64.ipynb will be more RAM friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the environment with your preferred method. We have tested the .yml conda load method on a conda python installation in Ubuntu 15.04, but not the others. We used pip to dump to requirements.txt which should be compatible with virtual env if you use python 3.6. \n",
    "\n",
    "To install something probably close enough to this environment you can use conda as follows:\n",
    "```\n",
    "conda create -n unirep python=3.6.0 tensorflow=1.3.0 jupyter pandas\n",
    "source activate unirep\n",
    "```\n",
    "\n",
    "The most important version requirement is **tensorflow 1.3.0**. The others are likely fungible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from unirep import babbler1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the babbler. You need to provide the batch size you will use and the path to the weight directory.\n",
    "batch_size = 12\n",
    "b = babbler1900(batch_size=batch_size, model_path=\"./1900_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The babbler needs to receive data in the correct format, a (batch_size, max_seq_len) matrix with integer values, where the integers correspond to an amino acid label at that position, and the end of the sequence is padded with 0s until the max sequence length to form a non-ragged rectangular matrix. We provide a formatting function to translate a string of amino acids into a list of integers with the correct codex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = \"MRKGEELFTGVVPILVELDGDVNGHKFSVRGEGEGDATNGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFARYPDHMKQHDFFKSAMPEGYVQERTISFKDDGTYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNFNSHNVYITADKQKNGIKANFKIRHNVEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSVLSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24,  1,  2,  4, 13,  6,  6, 21, 18,  8, 13, 16, 16, 14, 17, 21, 16,\n",
       "        6, 21,  5, 13,  5, 16,  9, 13,  3,  4, 18,  7, 16,  2, 13,  6, 13,\n",
       "        6, 13,  5, 15,  8,  9, 13,  4, 21,  8, 21,  4, 18, 17, 11,  8,  8,\n",
       "       13,  4, 21, 14, 16, 14, 20, 14,  8, 21, 16,  8,  8, 21,  8, 19, 13,\n",
       "       16, 10, 11, 18, 15,  2, 19, 14,  5,  3,  1,  4, 10,  3,  5, 18, 18,\n",
       "        4,  7, 15,  1, 14,  6, 13, 19, 16, 10,  6,  2,  8, 17,  7, 18,  4,\n",
       "        5,  5, 13,  8, 19,  4,  8,  2, 15,  6, 16,  4, 18,  6, 13,  5,  8,\n",
       "       21, 16,  9,  2, 17,  6, 21,  4, 13, 17,  5, 18,  4,  6,  5, 13,  9,\n",
       "       17, 21, 13,  3,  4, 21,  6, 19,  9, 18,  9,  7,  3,  9, 16, 19, 17,\n",
       "        8, 15,  5,  4, 10,  4,  9, 13, 17,  4, 15,  9, 18,  4, 17,  2,  3,\n",
       "        9, 16,  6,  5, 13,  7, 16, 10, 21, 15,  5,  3, 19, 10, 10,  9,  8,\n",
       "       14, 17, 13,  5, 13, 14, 16, 21, 21, 14,  5,  9,  3, 19, 21,  7,  8,\n",
       "       10,  7, 16, 21,  7,  4,  5, 14,  9,  6,  4,  2,  5,  3,  1, 16, 21,\n",
       "       21,  6, 18, 16,  8, 15, 15, 13, 17,  8,  3, 13,  1,  5,  6, 21, 19,\n",
       "        4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(b.format_seq(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a function that will check your Amino Acid sequences don't contain any characters which will break the UniRep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.is_valid_seq(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are into it, you could do your own data flow as long as you ensure that the data format is obeyed. Alternatively, you could use the data flow we implemented for babbler training, which happens in the tensorflow graph. It reads from a file of integer sequences, shuffles them around, collects them into groups of similar length (to minimize padding waste) and pads them to the max_length. We'll show you how to do that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before you can train your model, sequences need to be saved in the correct format\n",
    "# suppose we have a new-line seperated file of AA sequences, seqs.txt, and we want to format them.\n",
    "with open(\"seqs.txt\", \"r\") as source:\n",
    "    with open(\"formatted.txt\", \"w\") as destination:\n",
    "        for i,seq in enumerate(source):\n",
    "            seq = seq.strip()\n",
    "            if b.is_valid_seq(seq):\n",
    "                formatted = \",\".join(map(str,b.format_seq(seq)))\n",
    "                destination.write(formatted)\n",
    "                destination.write('\\n')\n",
    "            else:\n",
    "                raise ValueError(f\"Sequence {i} is not a valid sequence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24,1,2,4,13,6,6,21,18,8,13,16,16,14,17,21,16,6,21,5,13,5,16,9,13,3,4,18,7,16,2,13,6,13,6,13,5,15,8,9,13,4,21,8,21,4,18,17,11,8,8,13,4,21,14,16,14,20,14,8,21,16,8,8,21,8,19,13,16,10,11,18,15,2,19,14,5,3,1,4,10,3,5,18,18,4,7,15,1,14,6,13,19,16,10,6,2,8,17,7,18,4,5,5,13,8,19,4,8,2,15,6,16,4,18,6,13,5,8,21,16,9,2,17,6,21,4,13,17,5,18,4,6,5,13,9,17,21,13,3,4,21,6,19,9,18,9,7,3,9,16,19,17,8,15,5,4,10,4,9,13,17,4,15,9,18,4,17,2,3,9,16,6,5,13,7,16,10,21,15,5,3,19,10,10,9,8,14,17,13,5,13,14,16,21,21,14,5,9,3,19,21,7,8,10,7,16,21,7,4,5,14,9,6,4,2,5,3,1,16,21,21,6,18,16,8,15,15,13,17,8,3,13,1,5,6,21,19,4\r\n"
     ]
    }
   ],
   "source": [
    "# This is what the integer format looks like\n",
    "!head -n1 formatted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by default format_seq does not include the stop symbol (25) at the end of the sequence. This is the correct behavior if you are trying to train a top model, but not if you are training a babbler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use my custom function to bucket, batch and pad sequences  from formatted.txt (which has the correct integer codex after calling)\n",
    "babbler.format_seq(). The bucketing occurs in the graph. \n",
    "\n",
    "What is bucketing? Specify a lower and upper bound, and interval. All sequences less than lower or greater than upper will be batched together. Interval defines the \"sides\" of buckets between these bounds. Don't pick a small interval for a small dataset because\n",
    "the function will just repeat a sequence if there are not enough to\n",
    "fill a batch. All batches are the size you passed when initializing the babbler.\n",
    "What else is this doing? \n",
    "- Shuffling the sequences by randomly sampling from a 10000 sequence buffer\n",
    "- Automatically padding the sequences with zeros so the returned batch is a perfect rectangle\n",
    "- Automatically repeating the dataset (you will need synthetic epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_op = b.bucket_batch_pad(\"formatted.txt\", interval=1000) # Large interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconveniently, this does not make it easy for a value to be associated with each sequence and not lost during shuffling. You can get around this by just prepending every integer sequence with the sequence label (eg, every sequence would be saved to the file as \"{brightness value}, 24, 1, 5,...\" and then you could just index out the first column after calling the bucket_op. Please reach out if you have questions on how to do this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now that we have the bucket_op, we can simply sess.run() it to get\n",
    "# a correctly formatted batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch = sess.run(bucket_op)\n",
    "    \n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look back and see that the batch_size we passed to __init__ is indeed 12, and the second dimension must be the longest sequence included in this batch. Now we have the data flow setup (note that as long as your batch looks like this, you don't need my flow), so we can proceed to implementing the graph. The module returns all the operations needed to feed in sequence and get out trainable representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_hidden, x_placeholder, batch_size_placeholder, seq_length_placeholder, initial_state_placeholder = b.get_rep_ops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_hidden should be a batch_size x rep_dim matrix\n",
    "# Lets say I want to train a basic feed-forward network as the top\n",
    "# model, doing regression with MSE loss, Adam optimizer\n",
    "y_placeholder = tf.placeholder(tf.float32, shape=[None,1], name=\"y\")\n",
    "initializer = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "\n",
    "with tf.variable_scope(\"top\"):\n",
    "    prediction = tf.contrib.layers.fully_connected(\n",
    "        final_hidden, 1, activation_fn=None, \n",
    "        weights_initializer=initializer,\n",
    "        biases_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_placeholder, prediction)\n",
    "    \n",
    "# You can specifically train the top model first\n",
    "learning_rate=.001\n",
    "top_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"top\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "top_only_step_op = optimizer.minimize(loss, var_list=top_variables)\n",
    "all_step_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([832, 905, 957, 239, 877, 361, 444, 361, 376, 576, 445, 342])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that one of the placeholder is seq_length_placeholder.\n",
    "# We need to compute the lengths of the sequences in each batch so\n",
    "# that we can index out the the correct final hidden.\n",
    "def nonpad_len(batch):\n",
    "    nonzero = batch > 0\n",
    "    lengths = np.sum(nonzero, axis=1)\n",
    "    return lengths\n",
    "\n",
    "nonpad_len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 1747.3558349609375\n",
      "Iteration 1: 1734.4261474609375\n",
      "Iteration 2: 1730.3966064453125\n",
      "Iteration 3: 1718.4832763671875\n",
      "Iteration 4: 1703.1844482421875\n",
      "Iteration 5: 1694.8883056640625\n",
      "Iteration 6: 1686.18994140625\n",
      "Iteration 7: 1671.3614501953125\n",
      "Iteration 8: 1712.8775634765625\n",
      "Iteration 9: 1656.8701171875\n",
      "Iteration 10: 1648.7679443359375\n",
      "Iteration 11: 1638.9505615234375\n",
      "Iteration 12: 1624.810546875\n",
      "Iteration 13: 1612.3206787109375\n",
      "Iteration 14: 1617.3511962890625\n",
      "Iteration 15: 1601.2879638671875\n",
      "Iteration 16: 1595.2999267578125\n",
      "Iteration 17: 1577.05419921875\n",
      "Iteration 18: 1646.646484375\n",
      "Iteration 19: 1566.82177734375\n",
      "Iteration 20: 1552.0030517578125\n",
      "Iteration 21: 1561.1708984375\n",
      "Iteration 22: 1529.3271484375\n",
      "Iteration 23: 1527.52783203125\n",
      "Iteration 24: 1522.0374755859375\n",
      "Iteration 25: 1515.23046875\n",
      "Iteration 26: 1490.3385009765625\n",
      "Iteration 27: 1498.66796875\n",
      "Iteration 28: 1580.8349609375\n",
      "Iteration 29: 1489.6761474609375\n",
      "Iteration 30: 1464.6995849609375\n",
      "Iteration 31: 1464.1376953125\n",
      "Iteration 32: 1457.23486328125\n",
      "Iteration 33: 1455.8006591796875\n",
      "Iteration 34: 1423.9365234375\n",
      "Iteration 35: 1431.3314208984375\n",
      "Iteration 36: 1418.0419921875\n",
      "Iteration 37: 1433.0318603515625\n",
      "Iteration 38: 1384.4541015625\n",
      "Iteration 39: 1511.7984619140625\n",
      "Iteration 40: 1401.0758056640625\n",
      "Iteration 41: 1359.9339599609375\n",
      "Iteration 42: 1377.5562744140625\n",
      "Iteration 43: 1361.3365478515625\n",
      "Iteration 44: 1371.6673583984375\n",
      "Iteration 45: 1334.4532470703125\n",
      "Iteration 46: 1341.0162353515625\n",
      "Iteration 47: 1460.5357666015625\n",
      "Iteration 48: 1333.4659423828125\n",
      "Iteration 49: 1306.9246826171875\n",
      "Iteration 50: 1313.5347900390625\n",
      "Iteration 51: 1299.1328125\n",
      "Iteration 52: 1293.13671875\n",
      "Iteration 53: 1298.4505615234375\n",
      "Iteration 54: 1266.3218994140625\n",
      "Iteration 55: 1272.5130615234375\n",
      "Iteration 56: 1241.7901611328125\n",
      "Iteration 57: 1271.1680908203125\n",
      "Iteration 58: 1393.7301025390625\n",
      "Iteration 59: 1249.079833984375\n",
      "Iteration 60: 1249.580322265625\n",
      "Iteration 61: 1194.9085693359375\n",
      "Iteration 62: 1236.640869140625\n",
      "Iteration 63: 1197.78955078125\n",
      "Iteration 64: 1223.2711181640625\n",
      "Iteration 65: 1194.9285888671875\n",
      "Iteration 66: 1183.3819580078125\n",
      "Iteration 67: 1189.517578125\n",
      "Iteration 68: 1336.1820068359375\n",
      "Iteration 69: 1170.7991943359375\n",
      "Iteration 70: 1176.0733642578125\n",
      "Iteration 71: 1154.3028564453125\n",
      "Iteration 72: 1136.48291015625\n",
      "Iteration 73: 1158.79638671875\n",
      "Iteration 74: 1152.9296875\n",
      "Iteration 75: 1092.9608154296875\n",
      "Iteration 76: 1112.3890380859375\n",
      "Iteration 77: 1133.156982421875\n",
      "Iteration 78: 1280.0684814453125\n",
      "Iteration 79: 1090.7322998046875\n",
      "Iteration 80: 1093.79052734375\n",
      "Iteration 81: 1088.967529296875\n",
      "Iteration 82: 1080.6844482421875\n",
      "Iteration 83: 1089.7222900390625\n",
      "Iteration 84: 1040.3294677734375\n",
      "Iteration 85: 1074.1373291015625\n",
      "Iteration 86: 1047.010986328125\n",
      "Iteration 87: 1230.1851806640625\n",
      "Iteration 88: 1039.7518310546875\n",
      "Iteration 89: 1046.2613525390625\n",
      "Iteration 90: 1041.1668701171875\n",
      "Iteration 91: 1015.7691040039062\n",
      "Iteration 92: 1013.26171875\n",
      "Iteration 93: 1005.4773559570312\n",
      "Iteration 94: 1011.553466796875\n",
      "Iteration 95: 996.6257934570312\n",
      "Iteration 96: 1006.452392578125\n",
      "Iteration 97: 961.6906127929688\n",
      "Iteration 98: 1172.8387451171875\n",
      "Iteration 99: 982.9785766601562\n"
     ]
    }
   ],
   "source": [
    "# toy example where we learn to predict 42 just training the top\n",
    "y = [[42]]*batch_size\n",
    "num_iters = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_iters):\n",
    "        batch = sess.run(bucket_op)\n",
    "        length = nonpad_len(batch)\n",
    "        loss_, __, = sess.run([loss, top_only_step_op],\n",
    "                             feed_dict={\n",
    "                                 x_placeholder: batch,\n",
    "                                 y_placeholder: y,\n",
    "                                 batch_size_placeholder: batch_size,\n",
    "                                 seq_length_placeholder:length,\n",
    "                                 initial_state_placeholder:b._zero_state\n",
    "                             }\n",
    "                             )\n",
    "        print(f\"Iteration {i}: {loss_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train both a top model and the mLSTM. This won't work on a 16G RAM laptop. Joint recurrent/ top model training has been tested on p3.2xlarge, which has 16G of GPU RAM and 64G of system RAM. To see a demonstration of joint training on your laptop, please run  unirep_tutorial_64_unit.ipynb, which is the same architecture and interface except for 64 hidden units in 4 stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = [[42]]*batch_siz\n",
    "num_iters = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_iters):\n",
    "        batch = sess.run(bucket_op)\n",
    "        length = nonpad_len(batch)\n",
    "        loss_, __, = sess.run([loss, all_step_op],\n",
    "                             feed_dict={\n",
    "                                 x_placeholder: batch,\n",
    "                                 y_placeholder: y,\n",
    "                                 batch_size_placeholder: batch_size,\n",
    "                                 seq_length_placeholder:length,\n",
    "                                 initial_state_placeholder:b._zero_state\n",
    "                             }\n",
    "                             )\n",
    "        print(f\"Iteration {i}: {loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
